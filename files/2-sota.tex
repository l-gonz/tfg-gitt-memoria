\chapter{Estado del arte}
\label{chap:sota}

\section{Revisión de la literatura}
\label{sec:lit-rev}

% Herramientas
% Estudios comparativos
% Métodos de medición

El impacto energético del aprendizaje automático ha empezado ha desarrollarse recientemente como una preocupación significativa debido a la creciente complejidad y escala de los modelos utilizados en este campo. A pesar de la mayor concienciación existente entorno al consumo de los grandes modelos computacionales, muchas de las investigaciones actuales se ocupan únicamente de obtener las mejores predicciones posibles, independientemente del coste computacional que esto suponga. Sin embargo, algunas investigaciones recientes sí han intentado cuantificar y mitigar el consumo energético asociado con el entrenamiento y la inferencia de modelos de aprendizaje automático, desarrollando herramientas que permitan a los científicos de datos medir las emisiones de sus modelos. \citeauthor{lottick2019}, \citeyear{lottick2019} \cite{lottick2019} fue uno de los primeros artículos en proponer una declaración sistematizada de las emisiones de carbono como parte del proceso de elección de modelos de aprendizaje, proponiendo un modelo de informe energético fácilmente accesible para desarrolladores sin necesidad de análisis a nivel industrial.

Varios estudios han reportado el consumo energético de modelos de aprendizaje automático, destacando el alto costo energético de los grandes modelos de lenguaje natural y visión por computador. Por ejemplo, un estudio de 2019 de \citeauthor{strubell2019nlp} \cite{strubell2019nlp} encontró que el entrenamiento de un modelo Transformer para búsqueda de arquitectura neuronal (NAS, una técnica para automatizar el diseño de redes neuronales artificiales) puede consumir tanta energía como el gasto de varios automóviles durante toda su vida útil. Otros estudios como \citeauthor{dodge2022cloud}, \citeyear{dodge2022cloud} \cite{dodge2022cloud} se centran en el gasto de la inteligencia artificial cuando los modelos son entrenados en instancias en la <<nube>> (centros de datos distribuidos), estudiando como pueden reducirse las emisiones de $CO_2$ considerando la elección de región geográfica y hora del día en la que se lleva a cabo el entrenamiento. Estos hallazgos han impulsado la investigación en técnicas de optimización y eficiencia, tales como la poda de redes neuronales, la cuantización de pesos, y el uso de hardware especializado como TPU (Tensor Processing Units) y FPGA (Field-Programmable Gate Arrays), que pueden reducir significativamente el consumo energético \cite{goel2020survey}.

A día de hoy existen un gran número de herramientas para medir el consumo de energía en los modelos de aprendizaje. Sin embargo, el uso de esta métrica no se ha generalizado todavía como parte del proceso. En \citeauthor{eva2019review}, \citeyear{eva2019review} \cite{eva2019review} se hace una revisión de varias de estas herramientas y los métodos más destacados para estimar el consumo. En ella se observa que se debe encontrar un equilibrio entre las herramientas que proporcionan un resultado detallado de la energía con una gran carga adicional en el desarrollo y aquellas que proporcionan un resultado muy aproximado pero de fácil disponibilidad. Son estas últimas las que serán más interesantes para integrar de forma sencilla en un proceso de elección de modelos de aprendizaje, al proporcionar información en tiempo real sobre los algoritmos utilizados.


%%%% HERRAMIENTAS

Entre los proyectos destacados se encuentra CodeCarbon \cite{codecarbon}, una iniciativa que se ha centrado en medir la huella de carbono de las actividades computacionales en aprendizaje automático mediante un paquete para Python fácilmente integrable con librerías existentes. Otras herramientas (\cite{getzner2023accuracy}) proponen un método de estimación sin necesidad de llegar a entrenar los modelos, a través de la recopilación previa de medidas de calidad de consumo energético en modelos de base. El \emph{Machine Learning Emissions Calculator} (MLCO2)\cite{lacoste2022mlco2} presenta otro modelo de medida basado en la localización, la red eléctrica y el tiempo utilizado por el servidor responsable del entrenamiento. Otras herramientas disponibles para Python incluyen \emph{experiment-impact-tracker} \cite{henderson2020tracker}, Carbontracker \cite{anthony2020carbontracker} y Eco2AI \cite{budennyy2022eco2ai}. Estos paquetes utilizan métodos similares de estimación de la energía utilizada por CPUs y GPUs y su correlación con las emisiones de carbono de acuerdo a la red eléctrica utilizada.

Los resultados obtenidos de estas investigaciones resaltan la necesidad de considerar la eficiencia energética como un criterio fundamental en el diseño y entrenamiento de modelos de aprendizaje automático. Numerosas iniciativas han mostrado que, mediante la optimización de código y la selección de infraestructuras más sostenibles, se pueden lograr reducciones significativas en el consumo de energía y emisiones de carbono. Otros estudios han demostrado que la implementación de modelos más eficientes no necesariamente compromete la precisión, sugiriendo que es posible alcanzar un balance entre rendimiento y sostenibilidad.

En conclusión, la literatura destaca la importancia de desarrollar métodos y tecnologías que reduzcan el consumo energético en el aprendizaje automático. La medición del consumo eléctrico o las emisiones de carbono son cruciales para aumentar la concienciación sobre este problema y proporcionar herramientas prácticas para medir y reducir la huella de carbono. A medida que la demanda de modelos grandes y complejos continúa creciendo, es imperativo que la comunidad de investigación se enfoque en soluciones sostenibles que equilibren la eficiencia energética con el rendimiento del modelo.

\section{Introducción al aprendizaje automático}
\label{sec:intro-ml}

El aprendizaje automático se ha consolidado como una disciplina fundamental dentro del campo de la ingeniería y la informática, debido a su capacidad para desarrollar sistemas que pueden aprender y mejorar a partir de la experiencia sin ser explícitamente programados. En el contexto de este Trabajo de Fin de Grado, se desarrollará una aplicación para medir el consumo energético de proyectos de aprendizaje automático, un aspecto crítico dado el creciente uso de estos modelos en diversas aplicaciones y su consecuente impacto ambiental. Para explicar su funcionamiento, es necesario describir primero los conceptos básicos de esta materia.

El aprendizaje automático puede dividirse principalmente en dos categorías: aprendizaje supervisado y no supervisado \cite{kelleher2020fundamentals}. En el aprendizaje supervisado, el modelo se entrena utilizando un conjunto de datos etiquetados, es decir, cada muestra del conjunto de datos está asociada con una etiqueta que representa el resultado esperado. Las tareas más comunes en este tipo de aprendizaje son la clasificación, donde el objetivo es asignar muestras a una de varias categorías predefinidas, y la regresión, donde se predice un valor continuo. 
En contraste, el aprendizaje no supervisado no utiliza etiquetas. En su lugar, el modelo intenta identificar patrones y estructuras inherentes en los datos. Las tareas típicas en este enfoque incluyen la agrupación (\textit{clustering}), donde el objetivo es agrupar muestras similares, y la reducción de dimensionalidad, que busca simplificar los datos manteniendo su estructura esencial. Este proyecto se centrará en tareas de clasificación mediante aprendizaje supervisado.

Dentro de las tareas de clasificación se pueden distinguir dos casos: la clasificación binaria y la multiclase. La clasificación multiclase es una técnica de aprendizaje automático supervisado en la que el objetivo es categorizar cada muestra de datos en una de tres o más clases posibles. A diferencia de la clasificación binaria, que se limita a dos clases, la clasificación multiclase es más compleja debido a varios factores. En primer lugar, el aumento en el número de clases incrementa la dificultad para separar las categorías correctamente, ya que el modelo debe aprender a distinguir entre múltiples fronteras de decisión. Además, el desequilibrio entre clases puede ser más pronunciado, lo que complica el entrenamiento del modelo. Por último, las métricas de evaluación se vuelven más complejas, ya que es necesario considerar el rendimiento del modelo en todas las clases para obtener una visión completa de su precisión y robustez. 

Entre los modelos más habituales en el aprendizaje automático \cite{hastie2009elements} se encuentran los árboles de decisión, los modelos de Naive Bayes, las máquinas de soporte vectorial (SVM por sus siglas en inglés) y las redes neuronales. Estas últimas, inspiradas en la estructura del cerebro humano, son particularmente relevantes en problemas complejos de reconocimiento de patrones y se destacan por su capacidad de aproximar funciones no lineales mediante la composición de varias capas de neuronas artificiales. Otros modelos, como los bosques aleatorios (Random Forests) y los modelos de potenciación de gradiente (Gradient Boosting Machines), también son ampliamente utilizados debido a su capacidad para mejorar la precisión a través de la combinación de múltiples predictores.

La calidad y estructura de los conjuntos de datos juegan un papel crucial en el éxito de los modelos de aprendizaje automático. Un conjunto de datos típico está compuesto por muestras (o instancias), cada una descrita por una serie de atributos (o características) y, en el caso de aprendizaje supervisado, asociada con una etiqueta, la variable objetivo. Los atributos pueden ser de diversos tipos: continuos, categóricos, binarios u ordinales.

Para evaluar el rendimiento de un modelo, los datos se dividen generalmente en un conjunto de entrenamiento y un conjunto de prueba. El conjunto de entrenamiento se utiliza para ajustar los parámetros del modelo, mientras que el conjunto de prueba se emplea para evaluar su capacidad de generalización a datos no vistos. Este proceso ocurre en varias etapas clave. Primero, los datos se recopilan y preprocesan para asegurar su calidad y pertinencia. Luego, el modelo se entrena utilizando el conjunto de entrenamiento, ajustando sus parámetros internos para minimizar el error en la predicción. Este ajuste se realiza mediante algoritmos de optimización, como el descenso del gradiente, que iterativamente modifica los parámetros para encontrar la configuración que produce el menor error. Finalmente, el modelo se utiliza para predecir la variable objetivo en los datos del conjunto de prueba y se evalúa comparando las predicciones con las etiquetas reales de los datos
\cite{watt2020machine}.

La precisión de un modelo de aprendizaje automático está influenciada por diversos factores. La calidad y cantidad de los datos de entrenamiento son cruciales; datos ruidosos o insuficientes pueden llevar a modelos inexactos. Además, la elección del modelo y sus hiperparámetros, como la profundidad de los árboles en un bosque aleatorio o la tasa de aprendizaje en un modelo de potenciación de gradiente, también juegan un papel determinante. La complejidad del modelo debe equilibrarse cuidadosamente para evitar el sobreajuste, donde el modelo se ajusta demasiado bien a los datos de entrenamiento y falla en generalizar a nuevos datos.
Otro problema común en el aprendizaje automático es el desequilibrio de clases, donde una clase puede estar significativamente subrepresentada en el conjunto de datos, afectando la precisión del modelo, que tiende a ignorar las clases minoritarias. Algunas estrategias para subsanar este problema son la penalización de las clases mayoritarias mediante la aplicación de pesos distintos a cada clase o la utilización de métodos de ensamblaje que no son sensibles al desequilibrio de clases \cite{fernandez2018learning}.

A medida que aumentan la escala y complejidad de los modelos, el consumo energético de los modelos de aprendizaje automático se volverá un aspecto más crítico. Modelos más complejos, como las redes neuronales profundas, requieren un mayor poder computacional y, por ende, un mayor consumo de energía durante el entrenamiento y la inferencia. Otros factores que afectan el consumo energético incluyen la arquitectura del modelo, la cantidad de datos, el hardware utilizado (como CPUs, GPUs o TPUs) y la eficiencia de los algoritmos de optimización. Medir y optimizar el consumo energético es esencial para desarrollar soluciones sostenibles y escalables, especialmente en aplicaciones a gran escala como el procesamiento de grandes volúmenes de datos o la inteligencia artificial en tiempo real \cite{eva2019review}.

Este proyecto se centrará en la implementación de una aplicación para medir el consumo energético de proyectos de aprendizaje automático. Dicha aplicación considerará varios aspectos técnicos y metodológicos del aprendizaje automático, incluyendo la selección y evaluación de modelos, la gestión de conjuntos de datos, y la mitigación de problemas comunes, con el objetivo de contribuir a la eficiencia energética y sostenibilidad en el desarrollo de estas tecnologías.

\section{Librerías empleadas}

\subsection{Entorno de desarrollo: Visual Studio Code y WSL}
\label{sec:dev-env}

VSCode es un editor de código fuente abierto altamente extensible que se ha convertido en una herramienta predilecta entre desarrolladores debido a su interfaz amigable, integración con Git y soporte para una amplia gama de lenguajes de programación y extensiones. WSL (Windows Subsystem for Linux) es una característica de Windows que permite ejecutar un entorno de Linux completo directamente sobre Windows sin la necesidad de una máquina virtual o sistemas de arranque dual \cite{barnes2021pro}.

Es posible combinar VSCode con WSL mediante una extensión específica ofrecida por VSCode para WSL que permite a los usuarios abrir carpetas y archivos directamente en el sistema de archivos de Linux, beneficiándose de las capacidades avanzadas de depuración y análisis de código que ofrece el editor. Además, la extensión proporciona a los desarrolladores acceso a un terminal de Linux real, donde pueden ejecutar y probar su código en el mismo entorno en el que se desplegará. 


\subsection{CodeCarbon}

CodeCarbon \cite{codecarbon}\cite{codecarbon-software} es un paquete creado con la intención de permitir a desarrolladores monitorizar las emisiones de dióxido de carbono ($CO_{2}$) producidas por aplicaciones en Inteligencia Artificial y modelos de Aprendizaje Automático, que surge de la motivación de contar con una forma de registrar las enormes cantidades de energía que el auge de la IA ha provocado en la industria. El incremento del rendimiento y la precisión de los modelos de Aprendizaje Automático que se ha producido en años recientes se ha logrado a cambio de la utilización de enormes cantidades de información para conseguir el aprendizaje de los patrones y características subyacentes. Así, los modelos más avanzados emplean cantidades significativas de poder computacional, entrenando en procesadores avanzados durante semanas o meses y consumiendo en el proceso una gran cantidad de energía. Dependiendo de la red eléctrica utilizada, este desarrollo puede comportar la emisión de grandes cantidades de gases de efecto invernadero como el $CO_{2}$.

CodeCarbon estima la huella de carbono de una aplicación medida como kilogramos de $CO_{2}$ equivalentes, o $CO_{2}eq$, una medida estandarizada utilizada para expresar la capacidad de calentamiento global de varios gases de efecto invernadero como la cantidad de $CO_{2}$ que causaría un impacto ambiental equivalente. Para tareas de computación, que emiten $CO_{2}$ por medio de la electricidad que están consumiendo y que es generada como parte de la red eléctrica (por ejemplo, mediante la quema de combustibles fósiles como el carbón) las emisiones de carbono se miden en kilogramos de $CO_{2}$ equivalentes por kilovatio-hora. De esta forma, las emisiones de dióxido de carbono totales se calculan como el producto de la intensidad de carbono de la electricidad utilizada para la computación y la energía consumida por la infraestructura.

La intensidad de carbono de la electricidad se calcula como la media ponderada de las emisiones de las distintas fuentes de energía usadas para generar electricidad, incluyendo combustibles fósiles y renovables. En la herramienta se asigna un valor conocido de dióxido de carbono emitido por kilovatio-hora generado para cada uno de los combustibles (carbón, petróleo y gas natural). Otras fuentes renovables o consideradas como de bajo carbono incluyen la energía solar, hidroeléctrica, biomasa o geotérmica. La intensidad de carbono de cada combustible individual se calcula en base a medidas de generación de carbono y electricidad en los Estados Unidos, y aplicadas de forma generalizada en el resto del mundo. Cada red eléctrica local incluye una mezcla distinta de fuentes de energía y tiene por lo tanto asignada una intensidad de carbono total particular.

\subsection{scikit-learn}

Scikit-learn \cite{scikit-learn} es un módulo desarrollado para Python que integra un amplio rango de algoritmos de aprendizaje automático de última generación para problemas tanto supervisados como no supervisados. Este paquete pretende llevar el aprendizaje automático a desarrolladores no especialistas mediante el uso de un lenguaje generalista de alto nivel. Se hace hincapié en la facilidad de uso, el rendimiento, la documentación y la consistencia de la API \cite{scikit-learn-api}. Tiene las mínimas dependencias necesarias y está distribuido bajo la licencia BSD, con el objetivo de incentivar su uso tanto en ambientes educativos como comerciales.

Scikit-learn expone una gran variedad de algoritmos de aprendizaje utilizando una interfaz consistente y orientada a la resolución de tareas, lo que permite una comparación sencilla entre distintos métodos de aprendizaje para una misma aplicación. Al depender del ecosistema científico de Python, puede ser integrado con facilidad en aplicaciones que se salgan del rango tradicional del análisis estadístico de datos. Además, los algoritmos, que han sido implementados en un lenguaje de alto nivel, pueden ser utilizados como bloques de construcción para desarrollar estrategias más complejas que se adecuen a cada caso particular \cite{vanderplas2022python}.

% \subsection{Python}
% \subsection{Pandas}
% \label{subsec:pandas}

\subsection{Matplotlib}
\label{subsec:matplotlib}

Matplotlib es una de las librerías más destacadas y utilizadas en el ecosistema de Python para la creación de gráficos y visualizaciones de datos. Esta herramienta permite a los desarrolladores y científicos de datos generar una amplia variedad de gráficos estáticos, animados e interactivos con relativa facilidad. Matplotlib se caracteriza por su flexibilidad y capacidad para crear visualizaciones de alta calidad y personalizables, que van desde simples gráficos de líneas y barras hasta complejas visualizaciones tridimensionales y de mapas de calor. Además, su integración con otras librerías populares de Python, como NumPy y pandas, facilita el proceso de análisis y visualización de datos, convirtiéndola en una opción preferida en ámbitos académicos y profesionales.

El diseño de Matplotlib sigue una filosofía similar a la de MATLAB, lo que hace que sea especialmente accesible para usuarios familiarizados con ese entorno. Sin embargo, a diferencia de MATLAB, Matplotlib es de código abierto y gratuito. Las capacidades avanzadas de esta librería incluyen la personalización detallada de todos los elementos del gráfico, la posibilidad de exportar gráficos en múltiples formatos (como PNG, PDF y SVG), y la creación de gráficos interactivos utilizando bibliotecas adicionales como mpld3 y Plotly. 

\subsection{Microsoft Azure}
\label{subsec:azure}

Microsoft Azure es una plataforma de servicios en la nube que ofrece una amplia gama de herramientas y recursos para el despliegue y gestión de aplicaciones y servicios. Entre sus múltiples servicios, Azure permite a los usuarios crear y gestionar máquinas virtuales (VMs) con diversas configuraciones de recursos, adaptándose a las necesidades específicas de cada proyecto. Esta capacidad es especialmente valiosa en el campo del aprendizaje automático, donde las tareas de entrenamiento y prueba de modelos requieren recursos computacionales significativos y variados. Con Azure, los desarrolladores e investigadores pueden seleccionar configuraciones específicas de CPU, GPU, memoria y almacenamiento para optimizar el rendimiento y costo de sus experimentos de aprendizaje automático. Microsoft Azure ofrece un programa especial para estudiantes llamado "Azure for Students", que proporciona acceso gratuito a una variedad de servicios en la nube. Este programa incluye un crédito inicial de \$100 USD para usar en cualquier servicio de Azure durante 12 meses, sin necesidad de una tarjeta de crédito para registrarse. 



\input{files/2.1-models}
\input{files/2.2-datasets}

%\cleardoublepage