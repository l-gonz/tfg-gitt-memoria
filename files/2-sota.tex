\chapter{Estado del arte}
\label{chap:sota}

\section{Revisión de la literatura}
\label{sec:lit-rev}

\todo[inline]{Añadir citas, hacer más específico, comprobar referencias}

El impacto energético del aprendizaje automático ha emergido como una preocupación significativa debido a la creciente complejidad y escala de los modelos utilizados en este campo. Investigaciones recientes han intentado cuantificar y mitigar el consumo energético asociado con el entrenamiento y la inferencia de modelos de aprendizaje automático. Entre los proyectos destacados se encuentra Codecarbon, una iniciativa que se ha centrado en medir y reducir la huella de carbono de las actividades computacionales en aprendizaje automático.

Los métodos empleados para medir el consumo energético en el aprendizaje automático varían en precisión y granularidad. Codecarbon, por ejemplo, utiliza una combinación de información sobre la carga de trabajo del CPU/GPU, la ubicación geográfica de los centros de datos, y las emisiones promedio de CO2 por kWh de electricidad en diferentes regiones. Este enfoque permite estimaciones en tiempo real del consumo energético y las emisiones de carbono asociadas a las tareas de aprendizaje automático. Otros métodos más directos incluyen el uso de medidores de energía que se conectan directamente a los dispositivos de hardware utilizados para el entrenamiento de modelos, proporcionando datos más precisos pero con un alcance limitado a configuraciones específicas.

Varios estudios han reportado el consumo energético de modelos de aprendizaje automático, destacando el alto costo energético de los grandes modelos de lenguaje natural y visión por computador. Por ejemplo, un estudio de Strubell et al. (2019) encontró que el entrenamiento de un solo modelo Transformer puede consumir tanta energía como el gasto anual de varios automóviles. Este hallazgo ha impulsado la investigación en técnicas de optimización y eficiencia, tales como la poda de redes neuronales, la cuantización de pesos, y el uso de hardware especializado como TPU (Tensor Processing Units) y FPGA (Field-Programmable Gate Arrays), que pueden reducir significativamente el consumo energético.

Los resultados obtenidos de estas investigaciones resaltan la necesidad urgente de considerar la eficiencia energética como un criterio fundamental en el diseño y entrenamiento de modelos de aprendizaje automático. La iniciativa Codecarbon ha mostrado que, mediante la optimización de código y la selección de infraestructuras más sostenibles, se pueden lograr reducciones significativas en el consumo de energía y emisiones de carbono. Otros estudios han demostrado que la implementación de modelos más eficientes no necesariamente compromete la precisión, sugiriendo que es posible alcanzar un balance entre rendimiento y sostenibilidad.

En el ámbito de los grandes modelos de aprendizaje, como los modelos de lenguaje (e.g., GPT-3), visión por computador y el aprendizaje en la nube, el impacto energético es particularmente elevado. Los grandes modelos de lenguaje, por ejemplo, requieren enormes cantidades de datos y recursos computacionales para su entrenamiento, lo que se traduce en un consumo energético considerable. Del mismo modo, las aplicaciones de visión por computador, especialmente aquellas que involucran redes convolucionales profundas (CNN), también son intensivas en energía. El aprendizaje en la nube agrava estos problemas al trasladar el consumo energético a los centros de datos, que pueden tener diferentes niveles de eficiencia y fuentes de energía.

Los grandes modelos de aprendizaje, como los modelos de lenguaje de gran escala (large language models, LLMs), han mostrado un impacto significativo en el consumo energético debido a su tamaño y complejidad. GPT-3, uno de los modelos más conocidos, cuenta con 175 mil millones de parámetros y requiere una cantidad masiva de energía para su entrenamiento. Los modelos de visión por computador, como las redes neuronales convolucionales (CNNs) y sus variantes más profundas, también consumen cantidades sustanciales de energía durante el entrenamiento y la inferencia.

El uso de aprendizaje en la nube, aunque ofrece flexibilidad y escalabilidad, puede tener implicaciones negativas para el consumo energético. Los centros de datos que alimentan estos servicios requieren grandes cantidades de electricidad, y su impacto depende en gran medida de la fuente de energía utilizada. Algunos centros de datos están migrando hacia fuentes de energía renovable para mitigar este impacto, pero la transición no es uniforme en todas las regiones.

En conclusión, la literatura destaca la importancia de desarrollar métodos y tecnologías que reduzcan el consumo energético en el aprendizaje automático. Proyectos como Codecarbon son cruciales para aumentar la conciencia sobre este problema y proporcionar herramientas prácticas para medir y reducir la huella de carbono. A medida que la demanda de modelos grandes y complejos continúa creciendo, es imperativo que la comunidad de investigación se enfoque en soluciones sostenibles que equilibren la eficiencia energética con el rendimiento del modelo.

\section{Introducción al aprendizaje automático}
\label{sec:intro-ml}

El aprendizaje automático se ha consolidado como una disciplina fundamental dentro del campo de la ingeniería y la informática, debido a su capacidad para desarrollar sistemas que pueden aprender y mejorar a partir de la experiencia sin ser explícitamente programados. En el contexto de este trabajo de fin de grado, se desarrollará una aplicación para medir el consumo energético de proyectos de aprendizaje automático, un aspecto crítico dado el creciente uso de estos modelos en diversas aplicaciones y su impacto ambiental.

El aprendizaje automático puede dividirse principalmente en dos categorías: aprendizaje supervisado y no supervisado. En el aprendizaje supervisado, el modelo se entrena utilizando un conjunto de datos etiquetados, es decir, cada muestra del conjunto de datos está asociada con una etiqueta que representa el resultado esperado. Las tareas más comunes en este tipo de aprendizaje son la clasificación, donde el objetivo es asignar muestras a una de varias categorías predefinidas, y la regresión, donde se predice un valor continuo. Por ejemplo, en un problema de clasificación, el modelo podría aprender a identificar correos electrónicos como "spam" o "no spam", mientras que en un problema de regresión, el modelo podría predecir el precio de una vivienda basada en características como el tamaño y la ubicación.

En contraste, el aprendizaje no supervisado no utiliza etiquetas. En su lugar, el modelo intenta identificar patrones y estructuras inherentes en los datos. Las tareas típicas en este enfoque incluyen la agrupación (clustering), donde el objetivo es agrupar muestras similares, y la reducción de dimensionalidad, que busca simplificar los datos manteniendo su estructura esencial. Por ejemplo, en la agrupación, un modelo podría agrupar clientes de un supermercado en segmentos basados en sus comportamientos de compra.

Entre los modelos más habituales en el aprendizaje automático se encuentran los árboles de decisión, los modelos de Naive Bayes, las máquinas de soporte vectorial y las redes neuronales. Las redes neuronales, inspiradas en la estructura del cerebro humano, son particularmente relevantes en problemas complejos de reconocimiento de patrones y se destacan por su capacidad de aproximar funciones no lineales mediante la composición de varias capas de neuronas artificiales. Otros modelos, como los bosques aleatorios (Random Forests) y los modelos de potenciación de gradiente (Gradient Boosting Machines), también son ampliamente utilizados debido a su capacidad para mejorar la precisión a través de la combinación de múltiples predictores.

La calidad y estructura de los conjuntos de datos juegan un papel crucial en el éxito de los modelos de aprendizaje automático. Un conjunto de datos típico está compuesto por muestras (o instancias), cada una descrita por una serie de atributos (o características) y, en el caso de aprendizaje supervisado, asociada con una etiqueta (o variable objetivo). Los atributos pueden ser de diversos tipos: continuos (por ejemplo, la altura), categóricos (por ejemplo, el género), binarios (por ejemplo, sí/no), o ordinales (por ejemplo, la clasificación de satisfacción de un cliente en una escala de 1 a 5).

% Para evaluar el rendimiento de un modelo, los datos se dividen generalmente en un conjunto de entrenamiento y un conjunto de prueba. El conjunto de entrenamiento se utiliza para ajustar los parámetros del modelo, mientras que el conjunto de prueba se emplea para evaluar su capacidad de generalización a datos no vistos. 
El proceso de aprendizaje de estos modelos implica varias etapas clave. Primero, los datos se recopilan y preprocesan para asegurar su calidad y pertinencia. Luego, el modelo se entrena utilizando el conjunto de entrenamiento, ajustando sus parámetros internos para minimizar el error en la predicción. Este ajuste se realiza mediante algoritmos de optimización, como el descenso del gradiente, que iterativamente modifica los parámetros para encontrar la configuración que produce el menor error. Finalmente, el modelo se evalúa utilizando el conjunto de prueba para verificar su capacidad de generalización.

La precisión de un modelo de aprendizaje automático está influenciada por diversos factores. La calidad y cantidad de los datos de entrenamiento son cruciales; datos ruidosos o insuficientes pueden llevar a modelos inexactos. Además, la elección del modelo y sus hiperparámetros, como la profundidad de los árboles en un bosque aleatorio o la tasa de aprendizaje en un modelo de potenciación de gradiente, también juegan un papel determinante. La complejidad del modelo debe equilibrarse cuidadosamente para evitar el sobreajuste, donde el modelo se ajusta demasiado bien a los datos de entrenamiento y falla en generalizar a nuevos datos.
% Dos problemas comunes en el aprendizaje automático son el sobreajuste, donde el modelo se ajusta demasiado bien a los datos de entrenamiento y no generaliza bien, y el desequilibrio de clases, donde una clase puede estar significativamente subrepresentada en el conjunto de datos, afectando la precisión del modelo.

El consumo energético de los modelos de aprendizaje automático es otro aspecto crítico, especialmente a medida que aumentan la escala y complejidad de los modelos. Modelos más complejos, como las redes neuronales profundas, requieren un mayor poder computacional y, por ende, un mayor consumo de energía durante el entrenamiento y la inferencia. Factores que afectan el consumo energético incluyen la arquitectura del modelo, la cantidad de datos, el hardware utilizado (como CPUs, GPUs o TPUs) y la eficiencia de los algoritmos de optimización. Medir y optimizar el consumo energético es esencial para desarrollar soluciones sostenibles y escalables, especialmente en aplicaciones a gran escala como el procesamiento de grandes volúmenes de datos o la inteligencia artificial en tiempo real.

En conclusión, este trabajo de fin de grado se centrará en la implementación de una aplicación para medir el consumo energético de proyectos de aprendizaje automático. Dicha aplicación considerará varios aspectos técnicos y metodológicos del aprendizaje automático, incluyendo la selección y evaluación de modelos, la gestión de conjuntos de datos, y la mitigación de problemas comunes, con el objetivo de contribuir a la eficiencia energética y sostenibilidad en el desarrollo de estas tecnologías avanzadas.

\section{Librerías empleadas}

\subsection{Entorno de desarrollo: Visual Studio Code y WSL}
\label{sec:dev-env}

\todo[inline]{Get better info from other tfg}

VSCode es un editor de código fuente abierto altamente extensible que se ha convertido en una herramienta predilecta entre desarrolladores debido a su interfaz amigable, integración con Git y soporte para una amplia gama de lenguajes de programación y extensiones. WSL (Windows Subsystem for Linux) es una característica de Windows que permite ejecutar un entorno de Linux completo directamente sobre Windows sin la necesidad de una máquina virtual o sistemas de arranque dual.

Es posible combinar VSCode con WSL mediante una extensión específica ofrecida por VSCode para WSL que permite a los usuarios abrir carpetas y archivos directamente en el sistema de archivos de Linux, beneficiándose de las capacidades avanzadas de depuración y análisis de código que ofrece el editor. Además, la extensión proporciona a los desarrolladores acceso a un terminal de Linux real, donde pueden ejecutar y probar su código en el mismo entorno en el que se desplegará. 


\subsection{Codecarbon}

CodeCarbon \cite{codecarbon}\cite{codecarbon-software} es un paquete creado con la intención de permitir a desarrolladores monitorizar las emisiones de dióxido de carbono ($CO_{2}$) producidas por aplicaciones en Inteligencia Artificial y modelos de Aprendizaje Automático, que surge de la motivación de contar con una forma de registrar las enormes cantidades de energía que el auge de la IA ha provocado en la industria. El incremento del rendimiento y la precisión de los modelos de Aprendizaje Automático que se ha producido en años recientes se ha logrado a cambio de la utilización de enormes cantidades de información para conseguir el aprendizaje de los patrones y características subyacentes. Así, los modelos más avanzados emplean cantidades significativas de poder computacional, entrenando en procesadores avanzados durante semanas o meses y consumiendo en el proceso una gran cantidad de energía. Dependiendo de la red eléctrica utilizada, este desarrollo puede comportar la emisión de grandes cantidades de gases de efecto invernadero como el $CO_{2}$.

CodeCarbon estima la huella de carbono de una aplicación medida como kilogramos de $CO_{2}$ equivalentes, o $CO_{2}eq$, una medida estandarizada utilizada para expresar la capacidad de calentamiento global de varios gases de efecto invernadero como la cantidad de $CO_{2}$ que causaría un impacto ambiental equivalente. Para tareas de computación, que emiten $CO_{2}$ por medio de la electricidad que están consumiendo y que es generada como parte de la red eléctrica (por ejemplo, mediante la quema de combustibles fósiles como el carbón) las emisiones de carbono se miden en kilogramos de $CO_{2}$ equivalentes por kilovatio-hora. De esta forma, las emisiones de dióxido de carbono totales se calculan como el producto de la intensidad de carbono de la electricidad utilizada para la computación y la energía consumida por la infraestructura.

La intensidad de carbono de la electricidad se calcula como la media ponderada de las emisiones de las distintas fuentes de energía usadas para generar electricidad, incluyendo combustibles fósiles y renovables. En la herramienta se asigna un valor conocido de dióxido de carbono emitido por kilovatio-hora generado para cada uno de los combustibles (carbón, petróleo y gas natural). Otras fuentes renovables o consideradas como de bajo carbono incluyen la energía solar, hidroeléctrica, biomasa o geotérmica. La intensidad de carbono de cada combustible individual se calcula en base a medidas de generación de carbono y electricidad en los Estados Unidos, y aplicadas de forma generalizada en el resto del mundo. Cada red eléctrica local incluye una mezcla distinta de fuentes de energía y tiene por lo tanto asignada una intensidad de carbono total particular.

% IMAGE: Global distribution of carbon intensity (carbonboard)
% TABLE: Carbon intensity by energy source (Codecarbon/Methodology)
% CITE: CodeCarbon documentation

% _opcional_ : explanation on zero value for low-carbon fuels
% _opcional_ : explanation on power consuption calculation by CPU

\subsection{Scikit-Learn}

Scikit-learn \cite{scikit-learn} es un módulo desarrollado para Python que integra un amplio rango de algoritmos de aprendizaje automático de última generación para problemas tanto supervisados como no supervisados. Este paquete pretende llevar el aprendizaje automático a desarrolladores no especialistas mediante el uso de un lenguaje generalista de alto nivel. Se hace hincapié en la facilidad de uso, el rendimiento, la documentación y la consistencia de la API \cite{scikit-learn-api}. Tiene las mínimas dependencias necesarias y está distribuido bajo la licencia BSD, con el objetivo de incentivar su uso tanto en ambientes educativos como comerciales.

Scikit-learn expone una gran variedad de algoritmos de aprendizaje utilizando una interfaz consistente y orientada a la resolución de tareas, lo que permite una comparación sencilla entre distintos métodos de aprendizaje para una misma aplicación. Al depender del ecosistema científico de Python, puede ser integrado con facilidad en aplicaciones que se salgan del rango tradicional del análisis estadístico de datos. Además, los algoritmos, que han sido implementados en un lenguaje de alto nivel, pueden ser utilizados como bloques de construcción para desarrollar estrategias más complejas que se adecuen a cada caso particular.

\subsection{Microsoft Azure}
\label{subsec:azure}

Microsoft Azure es una plataforma de servicios en la nube que ofrece una amplia gama de herramientas y recursos para el despliegue y gestión de aplicaciones y servicios. Entre sus múltiples servicios, Azure permite a los usuarios crear y gestionar máquinas virtuales (VMs) con diversas configuraciones de recursos, adaptándose a las necesidades específicas de cada proyecto. Esta capacidad es especialmente valiosa en el campo del aprendizaje automático, donde las tareas de entrenamiento y prueba de modelos requieren recursos computacionales significativos y variados. Con Azure, los desarrolladores e investigadores pueden seleccionar configuraciones específicas de CPU, GPU, memoria y almacenamiento para optimizar el rendimiento y costo de sus experimentos de aprendizaje automático. Microsoft Azure ofrece un programa especial para estudiantes llamado "Azure for Students", que proporciona acceso gratuito a una variedad de servicios en la nube. Este programa incluye un crédito inicial de \$100 USD para usar en cualquier servicio de Azure durante 12 meses, sin necesidad de una tarjeta de crédito para registrarse. 

\subsection{Matplotlib}

Matplotlib es una de las librerías más destacadas y utilizadas en el ecosistema de Python para la creación de gráficos y visualizaciones de datos. Esta herramienta permite a los desarrolladores y científicos de datos generar una amplia variedad de gráficos estáticos, animados e interactivos con relativa facilidad. Matplotlib se caracteriza por su flexibilidad y capacidad para crear visualizaciones de alta calidad y personalizables, que van desde simples gráficos de líneas y barras hasta complejas visualizaciones tridimensionales y de mapas de calor. Además, su integración con otras librerías populares de Python, como NumPy y pandas, facilita el proceso de análisis y visualización de datos, convirtiéndola en una opción preferida en ámbitos académicos y profesionales.

El diseño de Matplotlib sigue una filosofía similar a la de MATLAB, lo que hace que sea especialmente accesible para usuarios familiarizados con ese entorno. Sin embargo, a diferencia de MATLAB, Matplotlib es de código abierto y gratuito. Las capacidades avanzadas de esta librería incluyen la personalización detallada de todos los elementos del gráfico, la posibilidad de exportar gráficos en múltiples formatos (como PNG, PDF y SVG), y la creación de gráficos interactivos utilizando bibliotecas adicionales como mpld3 y Plotly. 


\input{files/notes/---models}
\input{files/notes/---datasets}

%\cleardoublepage