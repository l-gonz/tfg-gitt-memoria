\chapter{Experimentos y validación}
\label{chap:experimentos}

El objetivo de este capítulo es mostrar el funcionamiento de la aplicación en un caso de uso real en el que se tratará de extraer conclusiones generales acerca del consumo eléctrico de cada modelo y de si este consumo irá necesariamente acompañado de una mejora de los resultados de predicción.
Para ello se emplearán las herramientas descritas anteriormente para evaluar el consumo y el rendimiento de una serie de modelos formada por representantes de las principales familias de modelos de aprendizaje automático y recogidos en la sección~\ref{sec:models}. Estos modelos serán aplicados a los conjuntos de datos de distintas características definidos en la sección~\ref{sec:datasets}.

\section{Estructura de los experimentos}

Durante la validación de la aplicación se llevarán acabo dos experimentos distintos.
El primero examinará el consumo energético en base al modelo seleccionado. En esta sección se tomarán varias medidas de consumo y rendimiento por modelo y conjunto de datos en una máquina con unos recursos de procesamiento concretos para analizar que modelos consumen más que otros y que características de los conjuntos de datos hacen incrementar este consumo.
El segundo experimento seleccionará un único conjuntos de datos de tamaño mediano y tomará medidas de consumo en tres entornos con distintos recursos de procesamiento dedicados a la tarea de aprendizaje automático para observar el efecto de los recursos disponibles en el consumo energético de cada modelo.

A través de este análisis, se pretende obtener una comprensión profunda de cómo diferentes modelos de aprendizaje automático consumen energía bajo diversas condiciones de trabajo. Además se intentarán identificar patrones de menor consumo que puedan utilizarse como base para el diseño y la implementación de modelos más sostenibles y eficientes en el futuro.


\begin{figure}[H]
  \centerline{
     \includegraphics[width=\textwidth, keepaspectratio]{img/test-structure.jpg}
  }
  \caption{Esquema general a seguir durante los experimentos}
  \label{fig:esquema-experimentos}
\end{figure}

La figura~\ref{fig:esquema-experimentos} muestra un esquema general que seguirán ambos experimentos. En primer lugar, se plantearán los modelos a analizar, los conjuntos de datos en los que serán entrenados y los recursos la máquina en la que se realizará el entrenamiento. Posteriormente, se utilizará la función "\texttt{mlcost measure}" de la aplicación, especificando los modelos y datos escogidos. De esta manera se llevarán a cabo las tres fases del proceso de medición por cada combinación modelo-conjunto posible: limpieza de los datos, entrenamiento del modelo y validación de la calidad de las predicciones. Para finalizar, se recogerán los datos resultantes del paso anterior y se alimentarán a la función "\texttt{mlcost plot}", que producirá las gráficas necesarias para analizar las relaciones entre el rendimiento de los modelos y su consumo energético y emisiones producidas.

\section{Consumo energético en función del modelo\\ seleccionado}
\label{sec:test-1-models}

\subsubsection{Objetivos}

En esta sección se examinará el consumo energético una serie de modelos representativos aplicados a varios conjuntos de datos. El objetivo de este análisis será abordar las siguientes cuestiones clave:

\begin{itemize}
    \item Identificar los modelos con mayor consumo energético.
    \item Determinar los modelos cuyo consumo energético incrementa significativamente al aumentar el número de muestras.
    \item Evaluar que modelos ofrecen mejores predicciones con menor consumo energético.
\end{itemize}

Dónde sea posible, se tratará de analizar estas cuestiones de forma general y obtener conclusiones que sean extrapolables más allá de los conjuntos de datos concretos que se hayan medido. Sin embargo, debido a la gran cantidad de variables involucradas y a las variaciones de consumo presentes entre unos casos y otros, es posible en otros conjuntos de datos muy diferentes de los considerados se observen distintos patrones de consumo.

\subsubsection{Metodología}

Para analizar estas cuestiones todas las medidas de consumo serán tomadas con la aplicación desarrollada ejecutando en una misma máquina. Las características de la máquina utilizada están recogidas en la tabla~\ref{tab:caracteristicas-tecnicas}.

\begin{table}[h]
    \caption{Características técnicas de la máquina utilizada para tomar las medidas}
    \label{tab:caracteristicas-tecnicas}
    \renewcommand\arraystretch{1.4}
    \centering
    \begin{tabular}{m{0.3\textwidth} m{0.55\textwidth}}
        \toprule
         \textbf{Característica} & \textbf{Descripción} \\
         \midrule
         Modelo & Dell XPS 15 9500\\
         Sistema Operativo & Ubuntu 20.04.6 LTS x86\_64\\
         Procesador & Intel(R) Core(TM) i9-10885H CPU @ 2.40GHz\\
         Versión Python & 3.12.2\\
         Memoria & 7,63 GB\\
         \bottomrule
    \end{tabular}
\end{table}

Para cada modelo y conjunto de datos, se tomarán medidas de consumo y rendimiento utilizando validación cruzada con cinco iteraciones con un tamaño definido para los datos de testeo del 20\% del conjunto de datos. Esta técnica proporcionará una evaluación robusta y precisa tanto del comportamiento energético de los modelos como de su precisión y exactitud, ya que evitará en gran medida la presencia de valores atípicos y el riesgo de sobreajuste de los modelos. La aplicación será ejecutada con el siguiente comando para cada conjunto de datos distinto, en el cual \texttt{[dataset]} será sustituido por el archivo que contenga cada conjunto de datos. Adicionalmente, cualquiera de las opciones de lectura de datos descritas en la sección~\ref{sec:limpieza} podrá ser utilizada si el formato en el que se encuentren los datos lo requiere.
\begin{minted}{bash}
mlcost measure --log -cv 5 -d [dataset] [dataset-options]
\end{minted}

Al ejecutar este comando, el programa escribirá al terminal en primer lugar una descripción de las características técnicas de la máquina donde se realiza el entrenamiento, con la información mostrada en la tabla \ref{tab:caracteristicas-tecnicas}. A continuación, se imprimirá un resumen del preprocesamiento realizado en el conjunto de datos elegido, incluyendo las características que se usarán en el entrenamiento, el número de muestras en los conjuntos de entrenamiento y de prueba y la distribución de las clases permitidas.

Para obtener los resultados buscados, se deberá ejecutar el comando mencionado para todos los conjuntos a analizar. A continuación, se enumeran las opciones exactas utilizadas para tratar de forma adecuada cada uno de ellos. La salida del terminal con todos los resultados de la limpieza y preprocesado se encuentran recogida en el apéndice~\ref{app:preprocessing}.

\begin{minted}[tabsize=2,breaklines]{bash}
# Iris (conjunto por defecto)
mlcost measure --log -cv 5
# Ionosfera
mlcost measure --log -cv 5 -d data/ionosphere/ionosphere.data --no-header
# Autenticación de billetes
mlcost measure -d data/banknote/banknote.txt --no-header
# Fonemas
mlcost measure --log -cv 5 -openml -d phoneme
# Electroencefalograma
mlcost measure --log -cv 5 -openml -d eeg-eye-state
# Electricidad
mlcost measure --log -cv 5 -openml -d electricity
\end{minted}


\subsubsection{Resultados}

La ejecución de los comando anterior producirá un archivo tipo tabla de datos en formato CSV. La tabla~\ref{tab:medidas-1} recoge un extracto de los resultados obtenidos en el ordenador de referencia para seis conjuntos de datos distintos. El archivo completo está disponible en el repositorio de la aplicación\footnote{\url{https://github.com/l-gonz/tfg-gitt-mlcost/blob/main/out/all-models-laptop.csv}}.

\begin{table}[H]
\caption[Extracto de los resultados de entrenamiento]{Extracto de los resultados de entrenamiento.}

\label{tab:medidas-1}
\renewcommand\arraystretch{1.4}
\centerline{
\scalebox{0.75}{
\begin{tabular}{ >{\raggedright\arraybackslash}p{0.11\textwidth}  % Name
                 >{\raggedright\arraybackslash}p{0.1\textwidth}  % Model
                 >{\raggedleft\arraybackslash}p{0.11\textwidth} % CPU Load
                 >{\raggedleft\arraybackslash}p{0.11\textwidth}  % Acc
                 >{\raggedleft\arraybackslash}p{0.11\textwidth}  % Prec
                 >{\raggedleft\arraybackslash}p{0.09\textwidth}  % F-score
                 >{\raggedleft\arraybackslash}p{0.08\textwidth} % Recall
                 >{\raggedleft\arraybackslash}p{0.09\textwidth}  % Fit
                 >{\raggedleft\arraybackslash}p{0.07\textwidth} % Time
                 >{\raggedleft\arraybackslash}p{0.12\textwidth} % Emiss
                 >{\raggedleft\arraybackslash}p{0.1\textwidth}  % Ener
                 >{\raggedleft\arraybackslash}p{0.05\textwidth} % Samples
                 }
\toprule
\textbf{Dataset}     & \textbf{Modelo} & \textbf{CPU load (\%)} & \textbf{Accuracy} & \textbf{Precision} & \textbf{F-score} & \textbf{Recall} & \textbf{Fit time (s)}  & \textbf{Total (s)} & \textbf{Emisiones (kg)} & \textbf{Energía (kWh)}  & \textbf{N} \\ 
\midrule
Banknote    & Linear & 2.7     & 0.98    & 0.98  & 0.98 & 0.98 & 0.007             & 0.071 & 2.13E-07 & 1.10E-06 & 1372 \\
Banknote    & Linear & 2.7      & 0.97  & 0.97  & 0.97  & 0.97  & 0.006             & 0.071       & 2.13E-07  & 1.10E-06 & 1372     \\
Banknote    & Linear & 2.7     & 0.97      & 0.97      & 0.97    & 0.97 & 0.006             & 0.071      & 2.13E-07  & 1.10E-06 & 1372     \\
Banknote    & Linear & 2.7     & 0.99      & 0.99   & 0.99  & 0.99  & 0.005             & 0.071      & 2.13E-07  & 1.10E-06 & 1372     \\
Banknote    & Linear & 2.7     & 0.99      & 0.99      & 0.99 & 0.99  & 0.005             & 0.071      & 2.13E-07  & 1.10E-06 & 1372     \\
Banknote    & Forest & 2.7      & 0.99      & 0.99      & 0.99    & 0.99  & 0.184             & 1.429     & 3.27E-06  & 1.69E-05 & 1372     \\
Banknote    & Forest & 2.7    & 1.00  & 1.00      & 1.00    & 1.00   & 0.171             & 1.429  & 3.27E-06  & 1.69E-05 & 1372     \\
Banknote    & Forest & 2.7   & 0.99   & 0.99      & 0.99    & 0.99 & 0.154             & 1.429  & 3.27E-06  & 1.69E-05 & 1372     \\
Banknote    & Forest & 2.7   & 1.00   & 1.00      & 1.00    & 1.00  & 0.172             & 1.429  & 3.27E-06  & 1.69E-05 & 1372     \\
Banknote    & Forest & 2.7    & 1.00  & 1.00      & 1.00    & 1.00  & 0.158             & 1.429    & 3.27E-06  & 1.69E-05 & 1372     \\
\multicolumn{12}{ c }{$\dots$} \\
Electricity & Neural & 102.4         & 0.82      & 0.83      & 0.83    & 0.83          & 132.768           & 518.905          & 1.19E-03  & 6.15E-03 & 45312 \\
\bottomrule
\end{tabular}}}
\end{table}

Cada fila en la tabla corresponde a las medidas tomadas durante una iteración de entrenamiento de cada modelo por validación cruzada. Al haber escogido utilizar validación cruzada de cinco iteraciones, la tabla de resultados cuenta con cinco filas por modelo y conjunto de datos. Sin embargo, algunas de las medidas, como la carga del procesador, el número de muestras del conjunto de datos, el tiempo total de entrenamiento, las emisiones del proceso y la energía consumida, son tomadas de forma global al finalizar todas las iteraciones de entrenamiento de cada modelo y conjunto.
Para cada iteración individual se recogen las medidas estadísticas de exactitud, precisión, exhaustividad y valor-F calculadas. Además, la implementación de validación cruzada de \texttt{scikit-learn} proporciona una medida del tiempo de entrenamiento empleado en cada iteración (fit time). Este valor puede ser utilizado junto con las emisiones y el tiempo totales de todas las iteraciones para calcular las emisiones de cada iteración de entrenamiento, como muestra la ecuación~\ref{eq:fit-emissions}.

\begin{equation}
    E_1 = \frac{E_T}{t_T} \cdot t_1 \;,
    \label{eq:fit-emissions}
\end{equation}
\begin{conditions}
E_1   &   emisiones de una iteración (kg [$CO_{2}eq$]), \\
E_T   &   emisiones totales (kg [$CO_{2}eq$]), \\
t_T   &   tiempo total (s), \\
t_1   &   tiempo de entrenamiento de la iteración (s).
\end{conditions}

A partir de los resultados obtenidos se puede dibujar un diagrama de dispersión para visualizar cómo varían las emisiones. En la figura~\ref{fig:scatter-1} se ha utilizado el valor-F como medida de la calidad de las predicciones (eje Y), ya que es habitualmente más informativa en casos de distribuciones de clases no balanceadas. En el eje X, se han dibujado las emisiones en kilogramos de carbono equivalentes con una escala logarítmica. Los distintos conjuntos de datos analizados se indican con formas geométricas distintas de los marcadores y cada modelo se indica en un color distinto. Los conjuntos están ordenados en la leyenda de la parte inferior de la gráfica por número de muestras, de menor a mayor tamaño.

\begin{figure}[H]
  \centerline{
     \includegraphics[width=1.3\textwidth, keepaspectratio]{img/graph/4scatter-dataset-model.png}
  }
  \caption{Valor-F alcanzado por el modelo frente a las emisiones de carbono necesarias para entrenarlo, por modelo empleado y conjunto de datos utilizado.}
  \label{fig:scatter-1}
\end{figure}

Este tipo de representación de los resultados permite observar las relaciones entre cuatro variables al mismo tiempo. De esta forma, se puede observar como cambia la relación entre la puntuación del modelo y sus emisiones de carbono para todas las distintas combinaciones de conjunto de datos y modelo de aprendizaje. Analizando esta gráfica, se puede obtener una idea de que modelos consiguen mantener una buena puntuación con un consumo bajo en la mayoría de las ocasiones.

En primer lugar, cabe destacar que la mayoría de las predicciones tienen una puntuación que se encuentra por encima del 70\%. Sin embargo, en la mitad inferior de la gráfica~\ref{fig:scatter-1} destacan los resultados obtenidos para el conjunto del electroencefalograma (\emph{eeg-eye-state}). Este conjunto presenta un valor-F que llega a estar incluso por debajo del 50\% al utilizar el modelo Naive Bayes, lo que representa un resultado peor incluso que un predictor que simplemente eligiera la clase de cada muestra de forma aleatoria entre los dos valores posibles. Para este conjunto, también se obtienen resultados bastante deficientes con el modelo lineal y el de vector soporte, que se encuentran entre el 50 y el 70\%. Estos resultados podrían indicar un problema en la fase de limpieza y preprocesamiento de las características, que no actúa de manera adecuada para un caso específico que presente este conjunto de datos. En la gráfica se puede apreciar que modelos que suelen ser en general menos sensibles al utilizarse con conjuntos poco preparados, como el bosque aleatorio o las redes neuronales, consiguen obtener resultados más aceptables para el conjunto del electroencefalograma.

Para el resto de resultados que se encuentran en el rango superior del valor-F, se observa en general que las mayores variaciones vienen dadas por el modelo empleado, de forma que los resultados para un modelo particular para todos los conjuntos se encuentran ligeramente agrupados en una sección de la gráfica, con las emisiones aumentando a medida que se incrementa el tamaño del conjunto de datos. Sin embargo, para el caso específico del modelo de vector soporte, se observa una variación mucho más extrema de las emisiones con el aumento de tamaño del conjunto utilizado, a la vez que la puntuación obtenida en las predicciones disminuye considerablemente en comparación con los resultados obtenidos para los conjuntos más pequeños, que dominan la parte superior izquierda de la gráfica.

Otro caso de emisiones especialmente elevadas se produce al usar el modelo de redes neuronales que destaca por conseguir puntuaciones de valor-F bastante elevadas para todos los conjuntos de datos, incluso para los datos del electroencefalograma, pero a costa de una utilización de recursos que aumenta exponencialmente y produce las emisiones más elevadas de todos los modelos para los conjuntos más grandes y complejos.

Los últimos modelos fácilmente descartables en la mayoría de los casos son el modelo lineal y Naive Bayes, que presentan unos resultados muy similares. Estos modelos consigue mantener unas emisiones particularmente bajas para todos los conjuntos analizados. Sin embargo, presentan una tendencia claramente a la baja en el valor-F al aumentar la complejidad del conjunto de datos utilizado, incluso sin tener en cuenta los valores anómalos obtenidos en el conjunto del electroencefalograma.

De entre los modelos restantes, se distinguen claramente dos patrones distintos. Por un lado el modelo de vecinos más cercanos (\emph{neighbors}) sigue un patrón de emisiones similar al caso lineal, que comienza en la parte izquierda de la gráfica y consigue incrementar muy poco en emisiones al aumentar la complejidad del conjunto de datos. Sin embargo, a diferencia del caso lineal, el valor-F no se muestra tan afectado por el aumento del tamaño del conjunto y se mantiene entre el 80 y el 90\%.

El último caso se da con los modelos de bosque aleatorio y de potenciación de gradiente. Estos modelos se encuentran en la parte media de la gráfica en cuanto a emisiones, con resultados muy parecidos entre los dos modelos. Sin embargo, su puntuación de valor-F se mantiene particularmente elevada para todos los conjuntos de datos, sin muchas variaciones. Dentro de estos dos modelos, el de bosque aleatorio obtiene los mejores resultados, consiguiendo emisiones más reducidas para conjuntos de pequeño tamaño y manteniendo una mayor puntuación para los conjuntos de mayor tamaño.



%Marcar areas en la gráfica siguiendo la discusión para mejor visualización ? \\


Para observar estos resultados de una forma más clara, se puede presentar la información en forma de gráfico de líneas. Las figuras \ref{fig:line-emissions-samples} y \ref{fig:line-fscore-samples} muestran de forma independiente las variaciones de la cantidad de emisiones y del valor-F al aumentar el número de muestras del conjunto de datos. Para estas gráficas, se han utilizado la media de emisiones y puntuación medidas en todas las iteraciones para cada modelo y conjunto.

\begin{figure}[H]
  \centerline{
     \includegraphics[width=0.9\textwidth, keepaspectratio]{img/graph/line-nsamples-emission-log.png}
  }
  \caption{Evolución de las emisiones de carbono con el aumento de número de muestras del conjunto de datos}
  \label{fig:line-emissions-samples}
\end{figure}
\begin{figure}[H]
  \centerline{
     \includegraphics[width=0.9\textwidth, keepaspectratio]{img/graph/line-nsamples-fscore-loglin.png}
  }
  \caption{Evolución de las emisiones de carbono con el aumento de número de muestras del conjunto de datos}
  \label{fig:line-fscore-samples}
\end{figure}

En la figura \ref{fig:line-emissions-samples}, es fácilmente apreciable la gran velocidad de crecimiento de las emisiones con el aumento de complejidad que presentan parte de los modelos (redes neuronales y vector soporte). Este tipo de gráfica permite seleccionar modelos interesantes en cuanto a las emisiones de forma sencilla, observando la inclinación de cada línea y su punto de partida. En la figura \ref{fig:line-fscore-samples}, el mayor beneficio que ofrece es poder descartar modelos que no alcancen un mínimo de puntuación en todos los escenarios, dependiendo de las necesidades específicas de cada caso. De este modo, se observa que los modelos de vector soporte, lineal y Naive Bayes serían fácilmente descartables como los más adecuados en este experimento concreto. 

% Para resumir los resultados, de todos los modelos analizados, destacan especialmente el modelo de vecinos más cercanos y el de bosque aleatorio. El primero sobresale por sus bajas emisiones y mantiene un valor-F adecuado en todos los casos. El segundo logra una gran puntuación de valor-F, mientras que sus emisiones se mantienen contenidas, incluso cuando aumenta la complejidad. Cabe destacar el caso del modelo de red neuronal, que muestra claramente la importancia de considerar el consumo energético al elegir modelos. Aunque obtiene resultados ligeramente superiores en puntuación al modelo de vecinos más cercanos, sus emisiones son hasta cuatro órdenes de magnitud mayores. Esta información adicional permite tomar mejores decisiones al seleccionar que modelo utilizar para una aplicación concreta.

\subsubsection{Conclusiones}

En el experimento, se analizaron las emisiones de carbono y el rendimiento de varios modelos de aprendizaje automático utilizando validación cruzada con cinco iteraciones. Los resultados se exportaron a un archivo CSV, y se observaron variaciones en las emisiones y puntuaciones de valor-F. La figura de dispersión mostró cómo las emisiones y el valor-F varían entre los modelos y conjuntos de datos, destacando las diferencias significativas entre ellos.

El modelo de vecinos más cercanos se destacó por sus bajas emisiones y un valor-F constante entre 80\% y 90\%. El modelo de bosque aleatorio y el de potenciación de gradiente mostraron emisiones moderadas y un alto valor-F. Sin embargo, el modelo de red neuronal, aunque obtuvo puntuaciones elevadas, tuvo las emisiones más altas, especialmente con conjuntos de datos grandes. Los modelos lineal y Naive Bayes presentaron bajas emisiones, pero su valor-F disminuyó con la complejidad del conjunto de datos. Los resultados indican que la selección del modelo debe considerar tanto el rendimiento como las emisiones, siendo el modelo de vecinos más cercanos y el de bosque aleatorio opciones destacadas por su equilibrio entre ambos aspectos.


\section{Consumo energético en función de los recursos\\ disponibles}
\label{sec:test-2-resources}

\subsubsection{Objetivos}

En el experimento anterior se observó que el modelo utilizado en el entrenamiento determina en gran medida el consumo energético. Sin embargo, hay una dimensión más que se puede estudiar para intentar reducir este consumo: los recursos computacionales dedicados a la tarea. Al utilizar un ordenador con menores recursos, el proceso de entrenamiento sin duda llevará más tiempo que en una máquina con mayores recursos. Sin embargo, su consumo energético por unidad de tiempo será comparativamente menor.
 
En esta sección pretende analizar que configuración de recursos actuará de forma más eficiente durante el entrenamiento de un conjunto de datos de gran tamaño, atendiendo a las siguientes cuestiones:

 \begin{itemize}
    \item Si es posible compensar más tiempo con menos consumo.
    \item Si se puede encontrar alguna combinación de recursos especialmente eficiente.
    \item Como afectan los recursos empleados a la calidad de las predicciones.
\end{itemize}

\subsubsection{Metodología}

La característica principal a analizar de las máquinas utilizadas será el número de procesadores dedicados a las tareas de entrenamiento. En un ordenador de un solo núcleo, cada iteración de la validación cruzada debe realizarse secuencialmente. Sin embargo, al aumentar el número de núcleos del procesador, es posible realizar las tareas de forma paralela. Para utilizar este mecanismo, es necesario activarla indicando a \texttt{scikit-learn} la opción \texttt{n\_jobs=-1} para indicar el número de procesos a ejecutar en paralelo, donde -1 indica utilizar todos los disponibles.

Para analizar como afecta el número de procesadores disponibles al consumo energético, se tomarán medidas en máquinas con tres configuraciones diferentes. Estas máquinas serán desplegadas como máquinas virtuales en la nube, lo que facilitará controlar los recursos disponibles para cada configuración y ofrecerá un entorno similar de trabajo para todos los casos. La plataforma elegida para el despliegue es Microsoft Azure (sección~\ref{subsec:azure}), que ofrece una gran capacidad de personalización en los recursos desplegados y cuenta con una oferta de créditos gratuitos para el sector educativo que pueden ser empleados en minutos de ejecución de maquinas virtuales. La configuración elegida para cada una de las máquinas donde se tomarán las medidas está recogida en la tabla~\ref{tab:caracteristicas-tecnicas-multi}. Todas las máquinas utilizarán la misma versión de sistema operativo y el mismo tipo de procesador, con variaciones en el número de CPUs virtuales y la cantidad de memoria RAM disponible.

\begin{table}[H]
    \caption{Características técnicas de las máquina virtuales comparadas}
    \label{tab:caracteristicas-tecnicas-multi}
    \renewcommand\arraystretch{1.6}
    \centering
    \begin{tabular}{>{\raggedright\arraybackslash}m{0.2\textwidth}
                    >{\centering\arraybackslash}m{0.2\textwidth}
                    >{\centering\arraybackslash}m{0.2\textwidth}
                    >{\centering\arraybackslash}m{0.2\textwidth}
                    }
    \toprule
    \textbf{Característica}  
    & \textbf{Test 1}                             
    & \textbf{Test 2}                             
    & \textbf{Test 3} \\ \midrule
Sistema operativo     
    & \multicolumn{3}{c}{Ubuntu 20.04.6 LTS x86\_64} \\
Procesador            
    & \multicolumn{3}{c}{\begin{tabular}[c]{@{}c@{}}Intel(R) Xeon(R) Platinum 8272CL CPU @ 2.60GHz\end{tabular}} 
     \\
Versión Python                
    & \multicolumn{3}{c}{3.12.3}  \\
Configuración Azure               
    & B1s   & B2ms   & B4ms  \\
Memoria (GB)              
    & 0,8?   & 7,75  & 15,57  \\
vCPUs 
    & 1   & 2   & 4  \\
    \bottomrule
\end{tabular}
\end{table}

El proceso completo seguido para cada configuración de recursos puede resumirse en los siguientes pasos:
\begin{enumerate}
    \item Desplegar una máquina virtual en la nube con la configuración deseada.
    \item Conectarse a la máquina virtual mediante SSH.
    \item Descargar el repositorio del proyecto.
    \item Ejecutar el script \texttt{azure/install.sh}.
    \item Ejecutar el script \texttt{azure/parallel.sh}.
\end{enumerate}

Este proceso será similar para cada combinación de recursos elegida. En primer lugar es necesario desplegar la máquina virtual con una red virtual configurada que permita conexiones mediante \texttt{ssh}, para poder ser controlada por línea de comandos. Los nombres que asigna Azure a las combinaciones de recursos específicas utilizadas están recogidos en la tabla~\ref{tab:caracteristicas-tecnicas-multi}. Además, el Anexo \ref{ax:az-templates} contiene unas plantillas que se pueden utilizar dentro de Microsoft Azure para desplegar de forma automática máquinas virtuales con una configuración idéntica a las utilizadas para tomar las medidas en este experimento.

Una vez que la máquina virtual se ha inicializado, será posible conectarse a ella mediante el protocolo \texttt{ssh}. Para no tener que configurar claves de acceso, es posible conectarse directamente con un terminal en línea que ofrece Azure directamente en su portal en la página de configuración de la máquina virtual. Cuando la conexión se establezca, el siguiente paso es descargar el repositorio del proyecto para tener acceso al código de la aplicación y a los recursos de instalación incluidos.

% git clone https://github.com/l-gonz/tfg-gitt-mlcost.git --branch=azure tfg-gitt-mlcost-azure

A continuación, se ejecutará el script \texttt{azure/install.sh} ubicado en el repositorio del proyecto\footnote{\url{https://github.com/l-gonz/tfg-gitt-mlcost/blob/main/azure/install.sh}}. Este script instalará la aplicación MLCost y todas sus dependencias, tomará una serie de medidas de entrenamiento en un conjunto de datos concreto, y exportará los resultados para su posterior análisis.

% # Installation
% curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash
% az login

% sudo add-apt-repository ppa:deadsnakes/ppa -y
% sudo apt install -y python3.12
% sudo apt install -y python3.12-venv

% ghtoken=$(az keyvault secret show -n Github-Token --vault-name kv-mlcost --query "value" -o tsv)
% cd ~/tfg-gitt-mlcost-azure
% git remote set-url origin https://azure-machine:${ghtoken}@github.com/l-gonz/tfg-gitt-mlcost.git
% git config --local user.email "azure.machine@email.com"
% git config --local user.name "azure-machine"

% python3.12 -m venv .venv
% source .venv/bin/activate
% python3.12 -m pip install -r requirements.txt
% python3.12 -m pip install -e .

Por último, se ejecutará el script \texttt{azure/parallel.sh}\footnote{\url{https://github.com/l-gonz/tfg-gitt-mlcost/blob/main/azure/parallel.sh}}. Este archivo tomará dos medidas distintas. La primera simplemente medirá la puntuación y las emisiones de todos los modelos de aprendizaje en un conjunto de datos con validación cruzada de forma similar al experimento anterior. La segunda medida, sin embargo, utilizará la opción \texttt{--parallel} de la aplicación, que provoca que las distintas iteraciones de entrenamiento se ejecuten de forma paralela. Al finalizar el proceso de entrenamiento de todos los modelos, los archivos CSV con las medidas tomadas se exportarán automáticamente al repositorio de la aplicación para poder ser analizados fuera de la máquina virtual. Este proceso se repetirá para cada una de las tres máquinas virtuales desplegadas, obteniendo medidas de entrenamiento de seis casos en total. Los comandos utilizados en este script para realizar las medidas en cada una de las máquinas serán los siguientes:
\begin{minted}{bash}
mlcost measure --log -cv 5 --openml -d electricity
mlcost measure --log -cv 5 --openml -d electricity --parallel
\end{minted}

El conjunto de datos elegido para analizar este proceso será \emph{electricity} (ver \ref{subsec:dataset-electricity}), que contiene alrededor de 45K muestras. Este tamaño moderado permite ejecutar los experimentos en un tiempo razonable del orden de pocos minutos por modelo. Al mismo tiempo, tiene un tamaño suficiente para que las diferencias de consumo generadas por las distintas configuraciones de recursos sean visibles. 


\subsubsection{Resultados}

Tras la ejecución de las seis mediciones tomadas los resultados se exportan a un archivo CSV de la misma manera que en el experimento anterior. El archivo completo con los resultados obtenidos está disponible en el repositorio de la aplicación\footnote{\url{https://github.com/l-gonz/tfg-gitt-mlcost/blob/main/out/output-parallel-azure-electricity.csv}}. En este caso, en vez de múltiples conjuntos de datos, se comparan las distintas configuraciones utilizadas. La figura~\ref{fig:4-2-non-average-scatter} representa todas las medidas obtenidas con sus puntuaciones de valor-F y emisiones producidas por iteración de entrenamiento. A diferencia de en el experimento anterior, ahora las distintas medidas obtienen resultados muy similares para cada iteración, por lo que los puntos de la gráfica se encuentran muy agrupados. Esto se debe a que la configuración de la máquina de entrenamiento produce un efecto mucho menor en las emisiones que la complejidad del conjunto de datos a analizar. Además, al ejecutar el experimento en una máquina virtual dedicada únicamente a esta tarea, con muy pocos programas adicionales instalados, las iteraciones obtienen resultados mucho más consistentes, que apenas se desvían en emisiones o valor-F obtenido. Por esta razón, se observarán directamente los valores medios de emisiones y puntuación por modelo y configuración.

\begin{figure}[H]
  \centerline{
     \includegraphics[width=\textwidth, keepaspectratio]{img/graph/2-scat-emissions-non-average.png}
  }
  \caption{Valor-F alcanzado por el modelo frente a las emisiones de carbono necesarias para entrenarlo, por modelo empleado y configuración de la máquina utilizada.}
  \label{fig:4-2-non-average-scatter}
\end{figure}

La figura~\ref{fig:4-2-average-scatter-fit-emissions} muestra el mismo gráfico de dispersión con estos valores medios calculados. En primer lugar, se puede observar que el valor-F medio obtenido se mantiene completamente constante para todas las configuraciones analizadas, y solo depende del modelo utilizado. Por lo tanto, para este experimento se puede obviar este valor, ya que no hay diferencia con el análisis de la sección anterior.

\begin{figure}[H]
  \centerline{
     \includegraphics[width=\textwidth, keepaspectratio]{img/graph/2-scat-fit-emissions.png}
  }
  \caption{Distribución de las emisiones por hilo de ejecución}
  \label{fig:4-2-average-scatter-fit-emissions}
\end{figure}

En cuanto a las emisiones obtenidas por iteración , se puede apreciar que existe algún tipo de separación entre las diferentes máquinas. Estos cambios son difíciles de analizar cuando la diferencia entre emisiones es mucho más grande entre distintos modelos que entre distintas configuraciones de recursos. Para poder obtener una visión más concreta del efecto de los recursos empleados, se pueden calcular las emisiones por segundo de computo de forma global, teniendo en cuenta las emisiones totales del proceso de entrenamiento y el tiempo total empleado, en vez de las emisiones por cada iteración. Esta nueva medida será calculada siguiendo la formula~\ref{eq:emissions-per-second}.

\begin{equation}
E(t) = \dfrac{E_T}{t_T}  \;,
\label{eq:emissions-per-second}
\end{equation}
\begin{conditions}
E(t)  &   emisiones por segundo (kg [$CO_{2}eq$] / s), \\
E_T   &   emisiones totales (kg [$CO_{2}eq$]), \\
t_T   &   tiempo total (s).
\end{conditions}

A partir de los resultados obtenidos, se puede utilizar un gráfico de líneas para observar las variaciones por configuración de recursos, como muestra la figura~\ref{fig:4-2-line-energy-per-second}. Con esta representación, se puede observar claramente que las emisiones aumentan al utilizar paralelismo, y en mayor medida al utilizar más procesadores en paralelo. Este resultado corresponde con lo esperado, ya que una máquina con mayores recursos utilizados consumirá más energía. Es interesante destacar que no se encuentran grandes diferencias en emisiones entre las distintas configuraciones si no se usa paralelismo, lo que indica que no se están aprovechando todos los recursos disponibles.

\begin{figure}[H]
  \centerline{
     \includegraphics[width=\textwidth, keepaspectratio]{img/graph/2-line-energy-p-s.png}
  }
  \caption{Distribución de la energía por segundo}
  \label{fig:4-2-line-energy-per-second}
\end{figure}

Sin embargo, una medida de emisiones por segundo no tiene en cuenta el tiempo total empleado en el proceso de entrenamiento. Como ya se ha comentado anteriormente, el mayor consumo de energía de una configuración más potente podría ser compensado con creces al emplear un tiempo mucho menor en el entrenamiento. La figura~\ref{fig:4-2-line-total-emissions} muestra las emisiones totales por configuración. Aquí se pude observar que para todos los modelos, las opciones que utilizan procesamiento en paralelo tienen unas emisiones totales menores, siendo la diferencia más significativa con un mayor numero de procesadores utilizados (T3-P).

\begin{figure}[H]
  \centerline{
     \includegraphics[width=\textwidth, keepaspectratio]{img/graph/2-line-emissions.png}
  }
  \caption{Distribución de la energía por segundo}
  \label{fig:4-2-line-total-emissions}
\end{figure}
%%% Gráfico de barras ?? grouped by model, coloured by config

%%> ¿Cambiar y tomar medidas con -cv 8 para ser divisible entre número cpus de todas las configuraciones?

\subsubsection{Conclusiones}

En este experimento, se realizaron seis mediciones cuyos resultados fueron exportados a un archivo CSV. A diferencia del experimento anterior, las configuraciones de hardware utilizadas en este experimento mostraron resultados muy consistentes en términos de emisiones y valor-F, debido a la uniformidad de la máquina de entrenamiento y la dedicación exclusiva de la máquina virtual.

El análisis de los valores medios reveló que el valor-F medio se mantuvo constante independientemente de la configuración, dependiendo únicamente del modelo utilizado. Las emisiones por iteración variaron más entre diferentes modelos que entre configuraciones de recursos. Se observó que las emisiones aumentan con el uso de paralelismo y más procesadores en paralelo, como era de esperarse, dado que el uso de más recursos incrementa el consumo de energía. Sin embargo, no se encontraron grandes diferencias en emisiones entre configuraciones sin paralelismo. Finalmente, se constató que aunque una configuración más potente consume más energía por segundo, el menor tiempo de entrenamiento requerido compensa este consumo, resultando en menores emisiones totales para opciones con procesamiento en paralelo.

\clearpage